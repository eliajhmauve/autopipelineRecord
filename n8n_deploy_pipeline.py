#!/usr/bin/env python3
"""
n8n è‡ªå‹•åŒ–éƒ¨ç½²ç®¡é“
æ”¯æŒå¾æœ¬åœ°é–‹ç™¼ç’°å¢ƒè‡ªå‹•éƒ¨ç½²å·¥ä½œæµåˆ° n8n å¯¦ä¾‹

Usage:
    python3 n8n_deploy_pipeline.py deploy <JSON_FILE> [--activate] [--validate]
    python3 n8n_deploy_pipeline.py batch-deploy <DIRECTORY> [--activate] [--validate]
    python3 n8n_deploy_pipeline.py validate <JSON_FILE>
    python3 n8n_deploy_pipeline.py backup [--output-dir DIRECTORY]
    python3 n8n_deploy_pipeline.py sync <LOCAL_DIR> <REMOTE_BACKUP>
"""

import os
import sys
import json
import requests
import argparse
import glob
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import shutil
from pathlib import Path

# å˜—è©¦è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from env_loader import load_env_file
    load_env_file()
except ImportError:
    # å¦‚æœ env_loader ä¸å­˜åœ¨ï¼Œå˜—è©¦æ‰‹å‹•è¼‰å…¥ .env
    if os.path.exists('.env'):
        with open('.env', 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

class N8nDeployPipeline:
    def __init__(self):
        self.host_url = os.getenv('N8N_HOST_URL')
        self.api_key = os.getenv('N8N_API_KEY')

        if not self.host_url or not self.api_key:
            print("éŒ¯èª¤: è«‹è¨­å®šå¿…è¦çš„ç’°å¢ƒè®Šæ•¸")
            print("ğŸ’¡ è«‹ç¢ºä¿ .env æ–‡ä»¶åŒ…å« N8N_HOST_URL å’Œ N8N_API_KEY")
            print("   æˆ–æ‰‹å‹•è¨­å®šç’°å¢ƒè®Šæ•¸:")
            print("   export N8N_HOST_URL='your_host_url'")
            print("   export N8N_API_KEY='your_actual_api_key_here'")
            sys.exit(1)
        
        self.headers = {
            'X-N8N-API-KEY': self.api_key,
            'Content-Type': 'application/json'
        }
        
        self.host_url = self.host_url.rstrip('/')
        
        # éƒ¨ç½²çµ±è¨ˆ
        self.deploy_stats = {
            'created': 0,
            'updated': 0,
            'activated': 0,
            'errors': 0,
            'skipped': 0
        }
    
    def _make_request(self, method: str, endpoint: str, data: Optional[Dict] = None, params: Optional[Dict] = None) -> Dict:
        """ç™¼é€ HTTP è«‹æ±‚åˆ° n8n API"""
        url = f"{self.host_url}/api/v1{endpoint}"
        
        try:
            if method.upper() == 'GET':
                response = requests.get(url, headers=self.headers, params=params)
            elif method.upper() == 'POST':
                response = requests.post(url, headers=self.headers, json=data, params=params)
            elif method.upper() == 'PUT':
                response = requests.put(url, headers=self.headers, json=data, params=params)
            elif method.upper() == 'PATCH':
                response = requests.patch(url, headers=self.headers, json=data, params=params)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„ HTTP æ–¹æ³•: {method}")
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"API è«‹æ±‚å¤±æ•—: {e}")
    
    def validate_workflow(self, workflow_data: Dict) -> Tuple[bool, List[str]]:
        """é©—è­‰å·¥ä½œæµ JSON çµæ§‹"""
        errors = []
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_fields = ['name', 'nodes', 'connections']
        for field in required_fields:
            if field not in workflow_data:
                errors.append(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
        
        # æª¢æŸ¥ç¯€é»çµæ§‹
        if 'nodes' in workflow_data:
            nodes = workflow_data['nodes']
            if not isinstance(nodes, list):
                errors.append("nodes å¿…é ˆæ˜¯é™£åˆ—")
            else:
                for i, node in enumerate(nodes):
                    if not isinstance(node, dict):
                        errors.append(f"ç¯€é» {i} å¿…é ˆæ˜¯ç‰©ä»¶")
                        continue
                    
                    # æª¢æŸ¥ç¯€é»å¿…è¦æ¬„ä½
                    node_required = ['type', 'typeVersion', 'position', 'id', 'name']
                    for field in node_required:
                        if field not in node:
                            errors.append(f"ç¯€é» {i} ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
        
        # æª¢æŸ¥é€£æ¥çµæ§‹
        if 'connections' in workflow_data:
            connections = workflow_data['connections']
            if not isinstance(connections, dict):
                errors.append("connections å¿…é ˆæ˜¯ç‰©ä»¶")
        
        # æª¢æŸ¥å·¥ä½œæµåç¨±
        if 'name' in workflow_data:
            name = workflow_data['name']
            if not isinstance(name, str) or len(name.strip()) == 0:
                errors.append("å·¥ä½œæµåç¨±ä¸èƒ½ç‚ºç©º")
        
        return len(errors) == 0, errors
    
    def deploy_single_workflow(self, json_file: str, activate: bool = False, validate: bool = True) -> bool:
        """éƒ¨ç½²å–®å€‹å·¥ä½œæµ"""
        print(f"\nğŸ“ æ­£åœ¨è™•ç†æ–‡ä»¶: {json_file}")
        
        # è®€å– JSON æ–‡ä»¶
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
            self.deploy_stats['errors'] += 1
            return False
        except json.JSONDecodeError as e:
            print(f"âŒ JSON æ ¼å¼éŒ¯èª¤: {e}")
            self.deploy_stats['errors'] += 1
            return False
        
        workflow_name = workflow_data.get('name', 'æœªå‘½åå·¥ä½œæµ')
        print(f"ğŸ·ï¸  å·¥ä½œæµåç¨±: {workflow_name}")
        
        # é©—è­‰å·¥ä½œæµ
        if validate:
            print("ğŸ” æ­£åœ¨é©—è­‰å·¥ä½œæµçµæ§‹...")
            is_valid, validation_errors = self.validate_workflow(workflow_data)
            if not is_valid:
                print("âŒ å·¥ä½œæµé©—è­‰å¤±æ•—:")
                for error in validation_errors:
                    print(f"   - {error}")
                self.deploy_stats['errors'] += 1
                return False
            print("âœ… å·¥ä½œæµçµæ§‹é©—è­‰é€šé")
        
        try:
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå·¥ä½œæµ
            print("ğŸ” æª¢æŸ¥ç¾æœ‰å·¥ä½œæµ...")
            existing_workflows = self._make_request('GET', '/workflows')
            existing_workflow = None
            
            for wf in existing_workflows.get('data', []):
                if wf.get('name') == workflow_name:
                    existing_workflow = wf
                    break
            
            if existing_workflow:
                workflow_id = existing_workflow['id']
                print(f"ğŸ”„ ç™¼ç¾åŒåå·¥ä½œæµï¼Œæ­£åœ¨æ›´æ–° (ID: {workflow_id})")
                
                # æ›´æ–°ç¾æœ‰å·¥ä½œæµ
                result = self._make_request('PUT', f'/workflows/{workflow_id}', workflow_data)
                self.deploy_stats['updated'] += 1
                action = "æ›´æ–°"
            else:
                print("ğŸ†• å‰µå»ºæ–°å·¥ä½œæµ")
                
                # å‰µå»ºæ–°å·¥ä½œæµ
                result = self._make_request('POST', '/workflows', workflow_data)
                self.deploy_stats['created'] += 1
                action = "å‰µå»º"
            
            deployed_workflow = result.get('data', {})
            workflow_id = deployed_workflow.get('id')
            
            print(f"âœ… å·¥ä½œæµ{action}æˆåŠŸ! (ID: {workflow_id})")
            
            # å¦‚æœéœ€è¦å•Ÿç”¨å·¥ä½œæµ
            if activate and not deployed_workflow.get('active', False):
                print("ğŸ”„ æ­£åœ¨å•Ÿç”¨å·¥ä½œæµ...")
                try:
                    self._make_request('PATCH', f'/workflows/{workflow_id}', {"active": True})
                    print("âœ… å·¥ä½œæµå·²å•Ÿç”¨")
                    self.deploy_stats['activated'] += 1
                except Exception as e:
                    print(f"âš ï¸  å•Ÿç”¨å·¥ä½œæµå¤±æ•—: {e}")
            
            current_status = 'å•Ÿç”¨' if deployed_workflow.get('active', False) or activate else 'åœç”¨'
            print(f"ğŸ“Š ç•¶å‰ç‹€æ…‹: {current_status}")
            
            return True
            
        except Exception as e:
            print(f"âŒ éƒ¨ç½²å¤±æ•—: {e}")
            self.deploy_stats['errors'] += 1
            return False
    
    def batch_deploy(self, directory: str, activate: bool = False, validate: bool = True) -> None:
        """æ‰¹é‡éƒ¨ç½²ç›®éŒ„ä¸­çš„æ‰€æœ‰å·¥ä½œæµ"""
        print(f"ğŸ“‚ æ­£åœ¨æƒæç›®éŒ„: {directory}")
        
        # å°‹æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶
        json_files = glob.glob(os.path.join(directory, "*.json"))
        
        if not json_files:
            print("âŒ ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ° JSON æ–‡ä»¶")
            return
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(json_files)} å€‹ JSON æ–‡ä»¶")
        
        # é‡ç½®çµ±è¨ˆ
        self.deploy_stats = {key: 0 for key in self.deploy_stats}
        
        successful_deployments = 0
        
        for json_file in json_files:
            if self.deploy_single_workflow(json_file, activate, validate):
                successful_deployments += 1
        
        # é¡¯ç¤ºéƒ¨ç½²çµ±è¨ˆ
        print("\n" + "="*60)
        print("ğŸ“Š éƒ¨ç½²çµ±è¨ˆå ±å‘Š")
        print("="*60)
        print(f"ç¸½æ–‡ä»¶æ•¸: {len(json_files)}")
        print(f"æˆåŠŸéƒ¨ç½²: {successful_deployments}")
        print(f"å‰µå»ºæ–°å·¥ä½œæµ: {self.deploy_stats['created']}")
        print(f"æ›´æ–°ç¾æœ‰å·¥ä½œæµ: {self.deploy_stats['updated']}")
        print(f"å•Ÿç”¨å·¥ä½œæµ: {self.deploy_stats['activated']}")
        print(f"éŒ¯èª¤æ•¸é‡: {self.deploy_stats['errors']}")
        print(f"è·³éæ•¸é‡: {self.deploy_stats['skipped']}")
        
        if self.deploy_stats['errors'] > 0:
            print(f"\nâš ï¸  æœ‰ {self.deploy_stats['errors']} å€‹æ–‡ä»¶éƒ¨ç½²å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤è¨Šæ¯")
        else:
            print(f"\nğŸ‰ æ‰€æœ‰å·¥ä½œæµéƒ¨ç½²å®Œæˆ!")
    
    def backup_workflows(self, output_dir: str = "n8n_backup") -> None:
        """å‚™ä»½æ‰€æœ‰å·¥ä½œæµåˆ°æœ¬åœ°ç›®éŒ„"""
        print(f"ğŸ’¾ æ­£åœ¨å‚™ä»½å·¥ä½œæµåˆ°ç›®éŒ„: {output_dir}")
        
        # å‰µå»ºå‚™ä»½ç›®éŒ„
        Path(output_dir).mkdir(exist_ok=True)
        
        try:
            # ç²å–æ‰€æœ‰å·¥ä½œæµ
            result = self._make_request('GET', '/workflows')
            workflows = result.get('data', [])
            
            if not workflows:
                print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å·¥ä½œæµ")
                return
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(workflows)} å€‹å·¥ä½œæµ")
            
            backup_count = 0
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            for workflow in workflows:
                workflow_id = workflow.get('id')
                workflow_name = workflow.get('name', 'unnamed_workflow')
                
                # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                safe_name = "".join(c for c in workflow_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_name = safe_name.replace(' ', '_')
                
                filename = f"{safe_name}_{workflow_id}_{timestamp}.json"
                filepath = os.path.join(output_dir, filename)
                
                try:
                    # ç²å–å®Œæ•´çš„å·¥ä½œæµæ•¸æ“š
                    full_workflow = self._make_request('GET', f'/workflows/{workflow_id}')
                    workflow_data = full_workflow.get('data', {})
                    
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(workflow_data, f, indent=2, ensure_ascii=False)
                    
                    print(f"âœ… å·²å‚™ä»½: {workflow_name} -> {filename}")
                    backup_count += 1
                    
                except Exception as e:
                    print(f"âŒ å‚™ä»½å¤±æ•— {workflow_name}: {e}")
            
            print(f"\nğŸ‰ å‚™ä»½å®Œæˆ! æˆåŠŸå‚™ä»½ {backup_count} å€‹å·¥ä½œæµåˆ° {output_dir}")
            
        except Exception as e:
            print(f"âŒ å‚™ä»½éç¨‹å¤±æ•—: {e}")

def main():
    parser = argparse.ArgumentParser(description='n8n è‡ªå‹•åŒ–éƒ¨ç½²ç®¡é“')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # deploy å‘½ä»¤
    deploy_parser = subparsers.add_parser('deploy', help='éƒ¨ç½²å–®å€‹å·¥ä½œæµ')
    deploy_parser.add_argument('json_file', help='å·¥ä½œæµ JSON æ–‡ä»¶è·¯å¾‘')
    deploy_parser.add_argument('--activate', action='store_true', help='éƒ¨ç½²å¾Œè‡ªå‹•å•Ÿç”¨')
    deploy_parser.add_argument('--validate', action='store_true', default=True, help='éƒ¨ç½²å‰é©—è­‰å·¥ä½œæµ')
    
    # batch-deploy å‘½ä»¤
    batch_parser = subparsers.add_parser('batch-deploy', help='æ‰¹é‡éƒ¨ç½²ç›®éŒ„ä¸­çš„å·¥ä½œæµ')
    batch_parser.add_argument('directory', help='åŒ…å« JSON æ–‡ä»¶çš„ç›®éŒ„è·¯å¾‘')
    batch_parser.add_argument('--activate', action='store_true', help='éƒ¨ç½²å¾Œè‡ªå‹•å•Ÿç”¨æ‰€æœ‰å·¥ä½œæµ')
    batch_parser.add_argument('--validate', action='store_true', default=True, help='éƒ¨ç½²å‰é©—è­‰æ‰€æœ‰å·¥ä½œæµ')
    
    # validate å‘½ä»¤
    validate_parser = subparsers.add_parser('validate', help='é©—è­‰å·¥ä½œæµ JSON æ–‡ä»¶')
    validate_parser.add_argument('json_file', help='è¦é©—è­‰çš„ JSON æ–‡ä»¶è·¯å¾‘')
    
    # backup å‘½ä»¤
    backup_parser = subparsers.add_parser('backup', help='å‚™ä»½æ‰€æœ‰å·¥ä½œæµ')
    backup_parser.add_argument('--output-dir', default='n8n_backup', help='å‚™ä»½è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # åˆå§‹åŒ–éƒ¨ç½²ç®¡é“
    pipeline = N8nDeployPipeline()
    
    # åŸ·è¡Œå°æ‡‰çš„å‘½ä»¤
    try:
        if args.command == 'deploy':
            pipeline.deploy_single_workflow(args.json_file, activate=args.activate, validate=args.validate)
        elif args.command == 'batch-deploy':
            pipeline.batch_deploy(args.directory, activate=args.activate, validate=args.validate)
        elif args.command == 'validate':
            with open(args.json_file, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
            is_valid, errors = pipeline.validate_workflow(workflow_data)
            if is_valid:
                print("âœ… å·¥ä½œæµé©—è­‰é€šé")
            else:
                print("âŒ å·¥ä½œæµé©—è­‰å¤±æ•—:")
                for error in errors:
                    print(f"   - {error}")
                sys.exit(1)
        elif args.command == 'backup':
            pipeline.backup_workflows(args.output_dir)
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"åŸ·è¡Œå‘½ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
